{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbiW1L5NaPbwl5dDOgtgbq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RatanakamonS/DADS6003-2024-S_RTNKMN/blob/main/HW2(DADS6003)_6620422002_Ratanakamon_Somklang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6620422002 Ratanakamon Somklang"
      ],
      "metadata": {
        "id": "J02s8aklPK6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Implement SGD, Mini-Batch from https://github.com/ekaratnida/Applied-machine-learning/blob/master/Week03-MLR/Lab3.ipynb"
      ],
      "metadata": {
        "id": "jNz7fjOeq8eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "s-IJtXs1qm8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "c-d1GNQjTSlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(theta, x, y):\n",
        "    y_hat = x.dot(theta)\n",
        "    c = (1/len(y)) * np.sum((y_hat - y) ** 2)\n",
        "    return c"
      ],
      "metadata": {
        "id": "xqTo_TWrTVyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(alpha, x, y, ep=0.001, max_iter=10000):\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    N = x.shape[0]  # number of samples\n",
        "    theta = np.random.random((x.shape[1], 1))  # initial theta\n",
        "\n",
        "    # Iterate until convergence or reaching max iterations\n",
        "    while not converged:\n",
        "        for i in range(N):\n",
        "            xi = x[i, :].reshape(1, -1)  # Take the i-th row of x\n",
        "            yi = y[i]  # Take the i-th element of y\n",
        "\n",
        "            y_hat = xi.dot(theta)\n",
        "            diff = y_hat - yi\n",
        "\n",
        "            grad = xi.T.dot(diff)\n",
        "            theta = theta - alpha * grad\n",
        "\n",
        "        # Compute error after full pass through the dataset\n",
        "        J = cost_function(theta, x, y)\n",
        "\n",
        "        # Check for convergence\n",
        "        if J < ep or iter >= max_iter:\n",
        "            converged = True\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "VLx7OpDhTV8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Example data\n",
        "    x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "    y = np.array([5, 7, 9, 11]).reshape(-1, 1)  # Labels\n",
        "\n",
        "    # Add bias (intercept term) to x\n",
        "    x_b = np.c_[np.ones((x.shape[0], 1)), x]\n",
        "\n",
        "    alpha = 0.01  # Learning rate\n",
        "    # Train the model using SGD\n",
        "    theta = stochastic_gradient_descent(alpha, x_b, y, ep=0.001, max_iter=10000)\n",
        "    print(\"Theta = \", theta)\n",
        "\n",
        "    # Predict new value\n",
        "    xtest = np.array([[4, 9]])\n",
        "    xtest_b = np.c_[np.ones((xtest.shape[0], 1)), xtest]\n",
        "    y_p = xtest_b.dot(theta)\n",
        "    print(\"y predict = \", y_p)"
      ],
      "metadata": {
        "id": "Rax_1ayIrOUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "S_cU2fNPrH3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "RCvvQjFbUsk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(theta, x, y):\n",
        "    y_hat = x.dot(theta)\n",
        "    c = (1/len(y)) * np.sum((y_hat - y) ** 2)\n",
        "    return c"
      ],
      "metadata": {
        "id": "270eL7I-UsFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(alpha, x, y, batch_size=20, ep=0.001, max_iter=10000):\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    N = x.shape[0]  # Number of samples\n",
        "    theta = np.random.random((x.shape[1], 1))  # Initial theta\n",
        "\n",
        "    while not converged:\n",
        "        # Shuffle the data\n",
        "        indices = np.arange(N)\n",
        "        np.random.shuffle(indices)\n",
        "        x_shuffled = x[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        for start in range(0, N, batch_size):\n",
        "            end = min(start + batch_size, N)\n",
        "            x_batch = x_shuffled[start:end]\n",
        "            y_batch = y_shuffled[start:end]\n",
        "\n",
        "            y_hat = x_batch.dot(theta)\n",
        "            diff = y_hat - y_batch\n",
        "\n",
        "            grad = x_batch.T.dot(diff) / len(y_batch)\n",
        "            theta = theta - alpha * grad\n",
        "\n",
        "        # Compute error after full pass through the dataset\n",
        "        J = cost_function(theta, x, y)\n",
        "\n",
        "        # Check for convergence\n",
        "        if J < ep or iter >= max_iter:\n",
        "            converged = True\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "H_coL34sUr0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Example data\n",
        "    x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "    y = np.array([5, 7, 9, 11]).reshape(-1, 1)  # Labels\n",
        "\n",
        "    # Add bias (intercept term) to x\n",
        "    x_b = np.c_[np.ones((x.shape[0], 1)), x]\n",
        "\n",
        "    alpha = 0.01  # Learning rate\n",
        "    batch_size = 2  # Mini-batch size\n",
        "    # Train the model using Mini-batch Gradient Descent\n",
        "    theta = mini_batch_gradient_descent(alpha, x_b, y, batch_size=batch_size, ep=0.001, max_iter=10000)\n",
        "    print(\"Theta = \", theta)\n",
        "\n",
        "    # Predict new value\n",
        "    xtest = np.array([[4, 9]])\n",
        "    xtest_b = np.c_[np.ones((xtest.shape[0], 1)), xtest]\n",
        "    y_p = xtest_b.dot(theta)\n",
        "    print(\"y predict = \", y_p)"
      ],
      "metadata": {
        "id": "Ndv1QHjerPrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}